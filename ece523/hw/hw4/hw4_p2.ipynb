{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-Adaboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from random import sample\n",
    "import math\n",
    "import random\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class adaboost:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def sample(self, N, p):\n",
    "        random_sample = np.zeros(N)\n",
    "        p_estimate = np.zeros(len(p))\n",
    "        p_cdf = np.cumsum(p)\n",
    "        counts = np.zeros(len(p))\n",
    "\n",
    "        for n in range(N):\n",
    "            # generate a random number on [0,1]\n",
    "            x = np.random.rand()\n",
    "            random_sample[n] = np.where(((p_cdf > x)*1.0) == 1.)[0][0]\n",
    "            counts[int(random_sample[n])] += 1\n",
    "\n",
    "        p_estimate = counts/counts.sum()\n",
    "        return random_sample, p_estimate\n",
    "    \n",
    "    def weakLearn(self, D, dataSet, DT=None):\n",
    "        # DT model with depth one\n",
    "        clf_gini = DecisionTreeClassifier(criterion = \"gini\", random_state = 100, max_depth=1)\n",
    "        #random distribution for the samples\n",
    "        random.seed(10)\n",
    "        rDataSet = dataSet.sample(len(D), replace = True, weights = D)\n",
    "\n",
    "        X_train = rDataSet.iloc[:,0:4]\n",
    "        y_train = rDataSet.iloc[:,4]\n",
    "\n",
    "        #fitting the DT model\n",
    "        stump = clf_gini.fit(X_train, y_train)\n",
    "\n",
    "        return stump\n",
    "\n",
    "    \n",
    "    def fit(self, X, y, T):\n",
    "        \"\"\"\n",
    "        X: feature vector \n",
    "        y: labels\n",
    "        T: number of iterations\n",
    "        \"\"\"\n",
    "        self.trainingData = pd.DataFrame(X)\n",
    "        self.trainingData['Label'] = y\n",
    "        self.stumps = [] \n",
    "        self.alphas = []\n",
    "        \n",
    "        #Initially assign same weights to each records in the dataset\n",
    "        self.trainingData['weights'] = 1/(self.trainingData.shape[0])\n",
    "        \n",
    "        for i in range(T):\n",
    "            print(\"iteration {} ...\".format(i+1))\n",
    "            #create weak classifier\n",
    "            stump = self.weakLearn(self.trainingData['weights'], self.trainingData)\n",
    "            \n",
    "            #append stumps\n",
    "            self.stumps.append(stump)\n",
    "\n",
    "            #make a prediction with the weak model\n",
    "            y_pred = stump.predict(self.trainingData.iloc[:,0:4])\n",
    "            \n",
    "\n",
    "            #save the prediction\n",
    "            self.trainingData['pred'] = y_pred\n",
    "            \n",
    "            #find the misclassified samples\n",
    "            self.trainingData['misclassified'] = \\\n",
    "                                np.where(self.trainingData['Label'] == self.trainingData['pred'], 0, 1)\n",
    "          \n",
    "            \n",
    "            #calulating the error\n",
    "            e = sum(self.trainingData['misclassified'] * self.trainingData['weights'])\n",
    "\n",
    "            #calculation of alpha\n",
    "            alpha = 0.5*math.log((1-e)/e)\n",
    "            \n",
    "            self.alphas.append(alpha)\n",
    "\n",
    "            #update weights\n",
    "            new_weights = self.trainingData['weights']*np.exp(-1*alpha*self.trainingData['Label'] \\\n",
    "                                *self.trainingData['pred'])\n",
    "\n",
    "            #normalized weight\n",
    "            z = sum(new_weights)\n",
    "            normalized_weights = new_weights/z\n",
    "\n",
    "\n",
    "            #add the new weights\n",
    "            self.trainingData['weights'] = normalized_weights\n",
    "          \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make prediction using the fitted model\n",
    "        \"\"\"\n",
    "        stump_preds = np.array([stump.predict(X) for stump in self.stumps])\n",
    "        return np.sign(np.dot(np.asarray(self.alphas), stump_preds))\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "blood = pd.read_csv(\"blood.csv\", header=None)\n",
    "blood.iloc[:,-1:] = blood.iloc[:,-1:].replace(to_replace = [1,0], value=[1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(748, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = blood.iloc[:,0:4].values\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(748,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = blood.iloc[:,4].values\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = adaboost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 ...\n",
      "iteration 2 ...\n",
      "iteration 3 ...\n",
      "iteration 4 ...\n",
      "iteration 5 ...\n",
      "iteration 6 ...\n",
      "iteration 7 ...\n",
      "iteration 8 ...\n",
      "iteration 9 ...\n",
      "iteration 10 ...\n",
      "iteration 11 ...\n",
      "iteration 12 ...\n",
      "iteration 13 ...\n",
      "iteration 14 ...\n",
      "iteration 15 ...\n",
      "iteration 16 ...\n",
      "iteration 17 ...\n",
      "iteration 18 ...\n",
      "iteration 19 ...\n",
      "iteration 20 ...\n"
     ]
    }
   ],
   "source": [
    "obj.fit(X, y, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stump_preds = obj.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing accuracy to sklearnâ€™simplementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[543,  27],\n",
       "       [118,  60]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using the confusion matrix for evaluating the accuracy\n",
    "c=confusion_matrix(y, stump_preds)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.61497326203208"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accuracy\n",
    "(c[0,0]+c[1,1])/np.sum(c)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X\n",
    "y_train = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(n_estimators=20, random_state=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = AdaBoostClassifier(n_estimators=20, random_state=0)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8074866310160428"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
