{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbsvqB9mScxP"
   },
   "source": [
    "# Recurrent Neural Networks for Languange Modeling \n",
    "## Model modification \n",
    "\n",
    "The plan is to modify the model build by adding L1 and L2 regularizations to the GRU layer and increase the number of RNN from 1024 to 2048.\n",
    "Reultes:\n",
    "\n",
    "I noticed that the loss after 40th epoch the loss function was 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "E_BU79tZScxU"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Wj4r3yIqScxU",
    "outputId": "9a04688d-77b7-4eb0-afe1-e5510c4054b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "1122304/1115394 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Sk4oulEoScxV",
    "outputId": "60ae269d-30c9-4ed9-dace-ace7f68e6d04"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/root/.keras/datasets/shakespeare.txt'"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "w8BF0BL1ScxV",
    "outputId": "3fc07bfc-53f6-4f98-e76c-b53fa28e322f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "HrOVzoGYScxV"
   },
   "outputs": [],
   "source": [
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "ixYxu6ChScxW",
    "outputId": "1650d331-9738-43fc-ec3e-32a4f8a1c65e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "q2Kzs58jScxW",
    "outputId": "960c4d8e-1467-4f37-b69e-90a787b8ebbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "GUhGB3tcScxW",
    "outputId": "3e59e47c-8700-47b4-a718-1210e6bbf009"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twibhLVSScxW"
   },
   "source": [
    "# Process the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "uUn0k32OScxX",
    "outputId": "1484a842-7945-4234-d98c-dda18aab5b52"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The preprocessing.StringLookup layer can convert each character into \n",
    "#a numeric ID. It just needs the text to be split into tokens first.\n",
    "example_texts = ['abcdefg', 'xyz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "8kLvWbq9ScxX"
   },
   "outputs": [],
   "source": [
    "#Now create the preprocessing.StringLookup layer:\n",
    "ids_from_chars = preprocessing.StringLookup(\n",
    "    vocabulary=list(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "jIvaCDngScxX",
    "outputId": "9da4981a-fa62-47eb-b16a-64add5ac9f12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[41, 42, 43, 44, 45, 46, 47], [64, 65, 66]]>"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#It converts form tokens to character IDs, padding with 0\n",
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "sSNPv8fDScxX"
   },
   "outputs": [],
   "source": [
    "#invert this representation and recover human-readable strings\n",
    "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "MF1X1OZ0ScxX",
    "outputId": "15e7c939-91fa-4d5c-e32c-d89e62f4169b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This layer recovers the characters from the vectors of IDs, and returns them as a tf.RaggedTensor of characters\n",
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "rl7pqyN-ScxY"
   },
   "outputs": [],
   "source": [
    "#You can tf.strings.reduce_join to join the characters back into strings.\n",
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "WnRb-15gScxY",
    "outputId": "1979b0f4-18c0-45ee-8201-2c5b5c96de9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'abcdefg', b'xyz'], dtype=object)>"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_from_ids(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHSGFqyQScxY"
   },
   "source": [
    "# The prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "yxtI5kb_ScxY",
    "outputId": "6a6a9675-ac40-4702-bd90-4217e6333d4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([20, 49, 58, ..., 47, 10,  2])>"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create training examples and targets\n",
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "dZA5aQyHScxY"
   },
   "outputs": [],
   "source": [
    "#use the tf.data.Dataset.from_tensor_slices function to convert the text vector into \n",
    "#a stream of character indices.\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "sz-y4JnBScxY",
    "outputId": "c8166df8-bb3b-40b2-ea85-ed45a2924fe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "QkZOyV96ScxY"
   },
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "BJ14BU3cScxZ",
    "outputId": "e3b84527-1c23-4be2-a549-8c12f04f1183"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "#batch method lets you easily convert these individual characters \n",
    "#to sequences of the desired size.\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Qvdw0S4cScxZ",
    "outputId": "47ef4820-dd10-4db7-8a2c-afa45aff2318"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "#It's easier to see what this is doing if you join the tokens back into strings:\n",
    "for seq in sequences.take(5):\n",
    "  print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "lhteRYGVScxZ"
   },
   "outputs": [],
   "source": [
    "#function that takes a sequence as input, duplicates, and shifts it to align the input and \n",
    "#label for each timestep\n",
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "m7d3nG46ScxZ"
   },
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "bxTgMtk1ScxZ",
    "outputId": "e33662bd-91a4-41a9-f0d1-93b74ed2d5ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "keORTOOlScxZ"
   },
   "source": [
    "# Create training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "FrFj4D4tScxa",
    "outputId": "336fdd90-d211-4b52-eafa-689a00f004f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWMLn-vHScxa"
   },
   "source": [
    "# Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "xAyaESLGScxa"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 2048 #incease from 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "yVJn5y9MScxa"
   },
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   kernel_regularizer=tf.keras.regularizers.L1(0.0001), #add regularizer\n",
    "                                   activity_regularizer=tf.keras.regularizers.L2(0.0001), ##add regularizer\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "dcNMu_GWScxa"
   },
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "1sI3z4UqScxa",
    "outputId": "7cad3e93-be61-41ae-e3b6-29413c84196a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 67) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "3ig_69p3Scxa",
    "outputId": "e153e2d5-a531-4338-8612-a3c5b1b9335e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  17152     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    multiple                  14168064  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  137283    \n",
      "=================================================================\n",
      "Total params: 14,322,499\n",
      "Trainable params: 14,322,499\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "J4v0zjf5Scxb"
   },
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Z7IZm4EyScxb",
    "outputId": "ea12b21e-2924-4223-de3a-a7525a585b97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([52,  9,  2, 58,  0, 44,  2, 56,  6, 11,  0, 39, 29, 59, 56,  7, 60,\n",
       "       52, 25, 18, 57, 16, 11, 28, 53, 64, 21, 54, 42,  2, 10, 44, 28, 34,\n",
       "       35,  3, 33, 45, 59, 18, 49, 38, 53,  9, 53, 27,  5, 35, 15, 53, 62,\n",
       "       39, 30, 23, 38, 57,  9, 33, 10, 36,  7, 54, 52, 12,  5, 39, 38, 64,\n",
       "       52, 22, 57, 31,  1, 42, 63,  0, 25,  1,  2, 11, 60,  7, 51, 34, 30,\n",
       "        1,  6,  7, 20, 35, 11, 33, 38, 12, 51,  8, 13, 57, 31, 32])"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Guf2rCXTScxb",
    "outputId": "79117d1c-32c8-4c4c-aa37-d03d4f6b888f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b\"th Baptista ta'en,\\nThat none shall have access unto Bianca\\nTill Katharina the curst have got a husba\"\n",
      "\n",
      "Next Char Predictions:\n",
      " b\"l-\\nrd\\np&3YOsp'tlKDqB3NmxGnb\\n.dNTU SesDiXm-mM$UAmvYPIXq-S.V'nl:$YXxlHqQ[UNK]bwK[UNK]\\n3t'kTP[UNK]&'FU3SX:k,;qQR\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fctt66S8Scxc"
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "LhQupFX9Scxc"
   },
   "outputs": [],
   "source": [
    "#Attach an optimizer, and a loss function\n",
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "mA-gX3x4Scxc",
    "outputId": "8d02689e-b9e2-46b6-f278-8870391e7b14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 67)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         4.2048993\n"
     ]
    }
   ],
   "source": [
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "mean_loss = example_batch_loss.numpy().mean()\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "EeRRGzUZScxc",
    "outputId": "624d41ba-899b-4076-f115-51ce83725bcd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67.01385"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "t0riNjgWScxc"
   },
   "outputs": [],
   "source": [
    "#Configure the training procedure using the tf.keras.Model.compile method. Use tf.keras.optimizers.Adam with default \n",
    "#arguments and the loss function\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "puN08zJAScxc"
   },
   "source": [
    "# Configure checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "q3XDsL6gScxc"
   },
   "outputs": [],
   "source": [
    "#Use a tf.keras.callbacks.ModelCheckpoint to ensure that checkpoints are saved during training\n",
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tiFIcYUCScxd"
   },
   "source": [
    "# Execute the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "q3HQrefvScxd"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "zKM6syiPScxd",
    "outputId": "63be3ca7-f13e-4fd6-a956-fe479319dee5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "172/172 [==============================] - 30s 159ms/step - loss: 4.6363\n",
      "Epoch 2/20\n",
      "172/172 [==============================] - 29s 164ms/step - loss: 2.5367\n",
      "Epoch 3/20\n",
      "172/172 [==============================] - 30s 168ms/step - loss: 2.2997\n",
      "Epoch 4/20\n",
      "172/172 [==============================] - 31s 172ms/step - loss: 2.1452\n",
      "Epoch 5/20\n",
      "172/172 [==============================] - 31s 176ms/step - loss: 2.0310\n",
      "Epoch 6/20\n",
      "172/172 [==============================] - 32s 180ms/step - loss: 1.9454\n",
      "Epoch 7/20\n",
      "172/172 [==============================] - 32s 182ms/step - loss: 1.8738\n",
      "Epoch 8/20\n",
      "172/172 [==============================] - 32s 183ms/step - loss: 1.8186\n",
      "Epoch 9/20\n",
      "172/172 [==============================] - 32s 183ms/step - loss: 1.7724\n",
      "Epoch 10/20\n",
      "172/172 [==============================] - 32s 183ms/step - loss: 1.7284\n",
      "Epoch 11/20\n",
      "172/172 [==============================] - 32s 183ms/step - loss: 1.6892\n",
      "Epoch 12/20\n",
      "172/172 [==============================] - 32s 183ms/step - loss: 1.6509\n",
      "Epoch 13/20\n",
      "172/172 [==============================] - 32s 183ms/step - loss: 1.6161\n",
      "Epoch 14/20\n",
      "172/172 [==============================] - 32s 183ms/step - loss: 1.5840\n",
      "Epoch 15/20\n",
      "172/172 [==============================] - 32s 183ms/step - loss: 1.5507\n",
      "Epoch 16/20\n",
      "172/172 [==============================] - 33s 183ms/step - loss: 1.5194\n",
      "Epoch 17/20\n",
      "172/172 [==============================] - 32s 183ms/step - loss: 1.4825\n",
      "Epoch 18/20\n",
      "172/172 [==============================] - 32s 183ms/step - loss: 1.4470\n",
      "Epoch 19/20\n",
      "172/172 [==============================] - 32s 183ms/step - loss: 1.4064\n",
      "Epoch 20/20\n",
      "172/172 [==============================] - 32s 183ms/step - loss: 1.3621\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPjS8FW8Scxd"
   },
   "source": [
    "# Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v81QBNJGScxd"
   },
   "outputs": [],
   "source": [
    "#The following makes a single step prediction:\n",
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['', '[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wgWQZ5DsScxd"
   },
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4JieVeVdScxe",
    "outputId": "ab65e1c1-6d81-4aa4-fe58-8a4ede475a20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "The last of Warwick call.\n",
      "\n",
      "CAMILLO:\n",
      "Nay, your office is dead.\n",
      "\n",
      "LORTS:\n",
      "So doth the death, they have a hearten mouth:\n",
      "And how she'll sooner in his mother's.\n",
      "The foul sir's news.\n",
      "\n",
      "BIANCA:\n",
      "Adgend and grief say he is colder,\n",
      "For such a hand of her, of my heart\n",
      "And hell in sucken me Above a cleagures\n",
      "That you shall unto Lonnon o'er the land, whose hap that Clifford,\n",
      "Was sentence of my life; for he dishollow'd attenting music and\n",
      "personal, sir.\n",
      "\n",
      "GLOUCESTER:\n",
      "Sir Richard, Or little tongue\n",
      "For beying foot to such absolate.\n",
      "\n",
      "LEONTES:\n",
      "What willoub children how?\n",
      "\n",
      "ROMEO:\n",
      "I pray thee, my lord.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "It is not meet him well: you are dishonour'd between out\n",
      "Where'er the people of this iclan careless.\n",
      "Say that she lives.\n",
      "\n",
      "TRASALLE:\n",
      "I do become him; for I think, let me hear\n",
      "The wind sid little eye o' the people, who haviness\n",
      "my wag's faithful fearful long, and nothing else.'\n",
      "Hast thou behold it straight degree?\n",
      "\n",
      "HORTENSIO:\n",
      "For this affection given him hence,\n",
      "And he will muck in promise beaute \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 1.754654884338379\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKLVvYOwScxe"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14ErzlzgScxe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw4_p3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
